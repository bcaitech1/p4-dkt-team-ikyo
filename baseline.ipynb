{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv5EvIVPnz0y"
   },
   "source": [
    "# LSTM 활용한 베이스라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in /opt/conda/lib/python3.7/site-packages (1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wtJhitPznz06"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import easydict\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w3E-ACunz07"
   },
   "source": [
    "## 1. 데이터 로드 및 전처리 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "od9O-ttAnz08"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self,args):\n",
    "        self.args = args\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        \n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data\n",
    "\n",
    "    def split_data(self, data, ratio=0.9, shuffle=True, seed=0):\n",
    "        \"\"\"\n",
    "        split data into two parts with a given ratio.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            random.seed(seed) # fix to default seed 0\n",
    "            random.shuffle(data)\n",
    "\n",
    "        size = int(len(data) * ratio)\n",
    "        data_1 = data[:size]\n",
    "        data_2 = data[size:]\n",
    "\n",
    "        return data_1, data_2\n",
    "\n",
    "    def __save_labels(self, encoder, name):\n",
    "        le_path = os.path.join(self.args.asset_dir, name + '_classes.npy')\n",
    "        np.save(le_path, encoder.classes_)\n",
    "\n",
    "    def __preprocessing(self, df, is_train = True):\n",
    "        cate_cols = ['assessmentItemID', 'testId', 'KnowledgeTag']\n",
    "\n",
    "        if not os.path.exists(self.args.asset_dir):\n",
    "            os.makedirs(self.args.asset_dir)\n",
    "            \n",
    "        for col in cate_cols:\n",
    "            \n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            if is_train:\n",
    "                #For UNKNOWN class\n",
    "                a = df[col].unique().tolist() + ['unknown']\n",
    "                le.fit(a)\n",
    "                self.__save_labels(le, col)\n",
    "            else:\n",
    "                label_path = os.path.join(self.args.asset_dir,col+'_classes.npy')\n",
    "                le.classes_ = np.load(label_path)\n",
    "                \n",
    "                df[col] = df[col].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "\n",
    "            #모든 컬럼이 범주형이라고 가정\n",
    "            df[col]= df[col].astype(str)\n",
    "            test = le.transform(df[col])\n",
    "            df[col] = test\n",
    "            \n",
    "\n",
    "        def convert_time(s):\n",
    "            timestamp = time.mktime(datetime.strptime(s, '%Y-%m-%d %H:%M:%S').timetuple())\n",
    "            return int(timestamp)\n",
    "\n",
    "        df['Timestamp'] = df['Timestamp'].apply(convert_time)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def __feature_engineering(self, df):\n",
    "        #TODO\n",
    "        return df\n",
    "\n",
    "    def load_data_from_file(self, file_name, is_train=True):\n",
    "        csv_file_path = os.path.join(self.args.data_dir, file_name)\n",
    "        df = pd.read_csv(csv_file_path)#, nrows=100000)\n",
    "        df = self.__feature_engineering(df)\n",
    "        df = self.__preprocessing(df, is_train)\n",
    "\n",
    "        # 추후 feature를 embedding할 시에 embedding_layer의 input 크기를 결정할때 사용\n",
    "\n",
    "                \n",
    "        self.args.n_questions = len(np.load(os.path.join(self.args.asset_dir,'assessmentItemID_classes.npy')))\n",
    "        self.args.n_test = len(np.load(os.path.join(self.args.asset_dir,'testId_classes.npy')))\n",
    "        self.args.n_tag = len(np.load(os.path.join(self.args.asset_dir,'KnowledgeTag_classes.npy')))\n",
    "        \n",
    "        df = df.sort_values(by=['userID','Timestamp'], axis=0)\n",
    "        columns = ['userID', 'assessmentItemID', 'testId', 'answerCode', 'KnowledgeTag']\n",
    "        group = df[columns].groupby('userID').apply(\n",
    "                lambda r: (\n",
    "                    r['testId'].values, \n",
    "                    r['assessmentItemID'].values,\n",
    "                    r['KnowledgeTag'].values,\n",
    "                    r['answerCode'].values\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return group.values\n",
    "\n",
    "    def load_train_data(self, file_name):\n",
    "        self.train_data = self.load_data_from_file(file_name)\n",
    "\n",
    "    def load_test_data(self, file_name):\n",
    "        self.test_data = self.load_data_from_file(file_name, is_train= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-MQhPevnz08"
   },
   "source": [
    "## 2. 데이터 셋 / 데이터 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h29rn8YNnz09"
   },
   "outputs": [],
   "source": [
    "class DKTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, args):\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "\n",
    "        # 각 data의 sequence length\n",
    "        seq_len = len(row[0])\n",
    "\n",
    "        test, question, tag, correct = row[0], row[1], row[2], row[3]\n",
    "        \n",
    "\n",
    "        cate_cols = [test, question, tag, correct]\n",
    "\n",
    "        # max seq len을 고려하여서 이보다 길면 자르고 아닐 경우 그대로 냅둔다\n",
    "        if seq_len > self.args.max_seq_len:\n",
    "            for i, col in enumerate(cate_cols):\n",
    "                cate_cols[i] = col[-self.args.max_seq_len:]\n",
    "            mask = np.ones(self.args.max_seq_len, dtype=np.int16)\n",
    "        else:\n",
    "            mask = np.zeros(self.args.max_seq_len, dtype=np.int16)\n",
    "            mask[-seq_len:] = 1\n",
    "\n",
    "        # mask도 columns 목록에 포함시킴\n",
    "        cate_cols.append(mask)\n",
    "\n",
    "        # np.array -> torch.tensor 형변환\n",
    "        for i, col in enumerate(cate_cols):\n",
    "            cate_cols[i] = torch.tensor(col)\n",
    "\n",
    "        return cate_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    col_n = len(batch[0])\n",
    "    col_list = [[] for _ in range(col_n)]\n",
    "    max_seq_len = len(batch[0][-1])\n",
    "\n",
    "        \n",
    "    # batch의 값들을 각 column끼리 그룹화\n",
    "    for row in batch:\n",
    "        for i, col in enumerate(row):\n",
    "            pre_padded = torch.zeros(max_seq_len)\n",
    "            pre_padded[-len(col):] = col\n",
    "            col_list[i].append(pre_padded)\n",
    "\n",
    "\n",
    "    for i, _ in enumerate(col_list):\n",
    "        col_list[i] =torch.stack(col_list[i])\n",
    "    \n",
    "    return tuple(col_list)\n",
    "\n",
    "\n",
    "def get_loaders(args, train, valid):\n",
    "\n",
    "    pin_memory = False\n",
    "    train_loader, valid_loader = None, None\n",
    "    \n",
    "    if train is not None:\n",
    "        trainset = DKTDataset(train, args)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, num_workers=args.num_workers, shuffle=True,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "    if valid is not None:\n",
    "        valset = DKTDataset(valid, args)\n",
    "        valid_loader = torch.utils.data.DataLoader(valset, num_workers=args.num_workers, shuffle=False,\n",
    "                            batch_size=args.batch_size, pin_memory=pin_memory, collate_fn=collate)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyiplxY6nz0-"
   },
   "source": [
    "## 3. LSTM 기반의 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "aO72oKAgnz0-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "try:\n",
    "    from transformers.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "except:\n",
    "    from transformers.models.bert.modeling_bert import BertConfig, BertEncoder, BertModel    \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.hidden_dim,\n",
    "                            self.hidden_dim,\n",
    "                            self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, input):\n",
    "        test, question, tag, _, mask, interaction, _ = input\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # Embedding\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        X = self.comb_proj(embed)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(X, hidden)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "class LSTMATTN(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(LSTMATTN, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "        self.n_heads = self.args.n_heads\n",
    "        self.drop_out = self.args.drop_out\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.hidden_dim,\n",
    "                            self.hidden_dim,\n",
    "                            self.n_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.config = BertConfig( \n",
    "            3, # not used\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_hidden_layers=1,\n",
    "            num_attention_heads=self.n_heads,\n",
    "            intermediate_size=self.hidden_dim,\n",
    "            hidden_dropout_prob=self.drop_out,\n",
    "            attention_probs_dropout_prob=self.drop_out,\n",
    "        )\n",
    "        self.attn = BertEncoder(self.config)            \n",
    "    \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        h = h.to(self.device)\n",
    "\n",
    "        c = torch.zeros(\n",
    "            self.n_layers,\n",
    "            batch_size,\n",
    "            self.hidden_dim)\n",
    "        c = c.to(self.device)\n",
    "\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        test, question, tag, _, mask, interaction, _ = input\n",
    "\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # Embedding\n",
    "\n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "        \n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        X = self.comb_proj(embed)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.lstm(X, hidden)\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "                \n",
    "        extended_attention_mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        head_mask = [None] * self.n_layers\n",
    "        \n",
    "        encoded_layers = self.attn(out, extended_attention_mask, head_mask=head_mask)        \n",
    "        sequence_output = encoded_layers[-1]\n",
    "        \n",
    "        out = self.fc(sequence_output)\n",
    "\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Bert, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = self.args.hidden_dim\n",
    "        self.n_layers = self.args.n_layers\n",
    "\n",
    "        # Embedding \n",
    "        # interaction은 현재 correct으로 구성되어있다. correct(1, 2) + padding(0)\n",
    "        self.embedding_interaction = nn.Embedding(3, self.hidden_dim//3)\n",
    "        self.embedding_test = nn.Embedding(self.args.n_test + 1, self.hidden_dim//3)\n",
    "        self.embedding_question = nn.Embedding(self.args.n_questions + 1, self.hidden_dim//3)\n",
    "        self.embedding_tag = nn.Embedding(self.args.n_tag + 1, self.hidden_dim//3)\n",
    "\n",
    "        # embedding combination projection\n",
    "        self.comb_proj = nn.Linear((self.hidden_dim//3)*4, self.hidden_dim)\n",
    "\n",
    "        # Bert config\n",
    "        self.config = BertConfig( \n",
    "            3, # not used\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_hidden_layers=self.args.n_layers,\n",
    "            num_attention_heads=self.args.n_heads,\n",
    "            max_position_embeddings=self.args.max_seq_len          \n",
    "        )\n",
    "\n",
    "        # Defining the layers\n",
    "        # Bert Layer\n",
    "        self.encoder = BertModel(self.config)  \n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.args.hidden_dim, 1)\n",
    "       \n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        test, question, tag, _, mask, interaction, _ = input\n",
    "        batch_size = interaction.size(0)\n",
    "\n",
    "        # 신나는 embedding\n",
    "        \n",
    "        embed_interaction = self.embedding_interaction(interaction)\n",
    "        embed_test = self.embedding_test(test)\n",
    "        embed_question = self.embedding_question(question)\n",
    "        embed_tag = self.embedding_tag(tag)\n",
    "\n",
    "        embed = torch.cat([embed_interaction,\n",
    "        \n",
    "                           embed_test,\n",
    "                           embed_question,\n",
    "        \n",
    "                           embed_tag,], 2)\n",
    "\n",
    "        X = self.comb_proj(embed)\n",
    "\n",
    "        # Bert\n",
    "        encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask)\n",
    "        out = encoded_layers[0]\n",
    "        out = out.contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        preds = self.activation(out).view(batch_size, -1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEaAa6Prnz0_"
   },
   "source": [
    "## 4. 모델 훈련을 위한 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "r_wU37QGnz0_"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# 훈련을 하기 위한 세팅\n",
    "def get_optimizer(model, args):\n",
    "    if args.optimizer == 'adam':\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "    if args.optimizer == 'adamW':\n",
    "        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "    \n",
    "    # 모든 parameter들의 grad값을 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, args):\n",
    "    if args.scheduler == 'plateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5, mode='max', verbose=True)\n",
    "    elif args.scheduler == 'linear_warmup':\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=args.warmup_steps,\n",
    "                                                    num_training_steps=args.total_steps)\n",
    "    elif args.scheduler == 'cosine_warmup':\n",
    "        scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=args.warmup_steps,\n",
    "                                                    num_training_steps=args.total_steps)\n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(pred, target):\n",
    "    loss = nn.BCELoss(reduction=\"none\")\n",
    "    return loss(pred, target)\n",
    "\n",
    "def get_metric(targets, preds):\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    acc = accuracy_score(targets, np.where(preds >= 0.5, 1, 0))\n",
    "    return auc, acc\n",
    "\n",
    "def get_model(args):\n",
    "    \"\"\"\n",
    "    Load model and move tensors to a given devices.\n",
    "    \"\"\"\n",
    "    if args.model == 'lstm': model = LSTM(args)\n",
    "    model.to(args.device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 배치 전처리\n",
    "def process_batch(batch, args):\n",
    "    test, question, tag, correct, mask = batch\n",
    "    \n",
    "    # change to float\n",
    "    mask = mask.type(torch.FloatTensor)\n",
    "    correct = correct.type(torch.FloatTensor)\n",
    "\n",
    "    #  interaction을 임시적으로 correct를 한칸 우측으로 이동한 것으로 사용\n",
    "    #    saint의 경우 decoder에 들어가는 input이다\n",
    "    interaction = correct + 1 # 패딩을 위해 correct값에 1을 더해준다.\n",
    "    interaction = interaction.roll(shifts=1, dims=1)\n",
    "    interaction[:, 0] = 0 # set padding index to the first sequence\n",
    "    interaction = (interaction * mask).to(torch.int64)\n",
    "    # print(interaction)\n",
    "    # exit()\n",
    "    #  test_id, question_id, tag\n",
    "    test = ((test + 1) * mask).to(torch.int64)\n",
    "    question = ((question + 1) * mask).to(torch.int64)\n",
    "    tag = ((tag + 1) * mask).to(torch.int64)\n",
    "\n",
    "    # gather index\n",
    "    # 마지막 sequence만 사용하기 위한 index\n",
    "    gather_index = torch.tensor(np.count_nonzero(mask, axis=1))\n",
    "    gather_index = gather_index.view(-1, 1) - 1\n",
    "\n",
    "\n",
    "    # device memory로 이동\n",
    "\n",
    "    test = test.to(args.device)\n",
    "    question = question.to(args.device)\n",
    "\n",
    "\n",
    "    tag = tag.to(args.device)\n",
    "    correct = correct.to(args.device)\n",
    "    mask = mask.to(args.device)\n",
    "\n",
    "    interaction = interaction.to(args.device)\n",
    "    gather_index = gather_index.to(args.device)\n",
    "\n",
    "    return (test, question,\n",
    "            tag, correct, mask,\n",
    "            interaction, gather_index)\n",
    "\n",
    "\n",
    "# loss계산하고 parameter update!\n",
    "def compute_loss(preds, targets):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        preds   : (batch_size, max_seq_len)\n",
    "        targets : (batch_size, max_seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    loss = get_criterion(preds, targets)\n",
    "    #마지막 시퀀드에 대한 값만 loss 계산\n",
    "    loss = loss[:,-1]\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def update_params(loss, model, optimizer, args):\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "def save_checkpoint(state, model_dir, model_filename):\n",
    "    print('saving model ...')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)    \n",
    "    torch.save(state, os.path.join(model_dir, model_filename))\n",
    "\n",
    "def load_model(args):\n",
    "    model_path = os.path.join(args.model_dir, args.model_name)\n",
    "    print(\"Loading Model from:\", model_path)\n",
    "    load_state = torch.load(model_path)\n",
    "    model = get_model(args)\n",
    "\n",
    "    # 1. load model state\n",
    "    model.load_state_dict(load_state['state_dict'], strict=True)\n",
    "    print(\"Loading Model from:\", model_path, \"...Finished.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO_xFaJYnz1B"
   },
   "source": [
    "## 5. 전체 프로세스를 담당하는 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BMiIOHgJnz1D"
   },
   "outputs": [],
   "source": [
    "def run(args, train_data, valid_data):\n",
    "    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n",
    "    \n",
    "    # only when using warmup scheduler\n",
    "    args.total_steps = int(len(train_loader.dataset) / args.batch_size) * (args.n_epochs)\n",
    "    args.warmup_steps = args.total_steps // 10\n",
    "            \n",
    "    model = get_model(args)\n",
    "    optimizer = get_optimizer(model, args)\n",
    "    scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "    best_auc = -1\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "\n",
    "        print(f\"Start Training: Epoch {epoch + 1}\")\n",
    "        \n",
    "        ### TRAIN\n",
    "        train_auc, train_acc, train_loss = train(train_loader, model, optimizer, args)\n",
    "        \n",
    "        ### VALID\n",
    "        auc, acc, _, _ = validate(valid_loader, model, args)\n",
    "\n",
    "        ### TODO: model save or early stopping\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_auc\": train_auc, \"train_acc\":train_acc,\n",
    "                  \"valid_auc\":auc, \"valid_acc\":acc})\n",
    "        if auc >= best_auc:\n",
    "            best_auc = auc\n",
    "            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model_to_save.state_dict(),\n",
    "                },\n",
    "                args.model_dir, 'model.pt',\n",
    "            )\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= args.patience:\n",
    "                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n",
    "                break\n",
    "\n",
    "        # scheduler\n",
    "        if args.scheduler == 'plateau':\n",
    "            scheduler.step(best_auc)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer, args):\n",
    "    model.train()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input = process_batch(batch, args)\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        loss = compute_loss(preds, targets)\n",
    "        update_params(loss, model, optimizer, args)\n",
    "\n",
    "        if step % args.log_steps == 0:\n",
    "            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n",
    "        \n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "        \n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "        losses.append(loss)\n",
    "      \n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    loss_avg = sum(losses)/len(losses)\n",
    "    print(f'TRAIN AUC : {auc} ACC : {acc}')\n",
    "    return auc, acc, loss_avg\n",
    "    \n",
    "\n",
    "def validate(valid_loader, model, args):\n",
    "    model.eval()\n",
    "\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        targets = input[3] # correct\n",
    "\n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        targets = targets[:,-1]\n",
    "    \n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "            targets = targets.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            targets = targets.detach().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        total_targets.append(targets)\n",
    "\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    # Train AUC / ACC\n",
    "    auc, acc = get_metric(total_targets, total_preds)\n",
    "    \n",
    "    print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
    "\n",
    "    return auc, acc, total_preds, total_targets\n",
    "\n",
    "\n",
    "\n",
    "def inference(args, test_data):\n",
    "    \n",
    "    model = load_model(args)\n",
    "    model.eval()\n",
    "    _, test_loader = get_loaders(args, None, test_data)\n",
    "    \n",
    "    \n",
    "    total_preds = []\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input = process_batch(batch, args)\n",
    "\n",
    "        preds = model(input)\n",
    "        \n",
    "\n",
    "        # predictions\n",
    "        preds = preds[:,-1]\n",
    "        \n",
    "\n",
    "        if args.device == 'cuda':\n",
    "            preds = preds.to('cpu').detach().numpy()\n",
    "        else: # cpu\n",
    "            preds = preds.detach().numpy()\n",
    "            \n",
    "        total_preds+=list(preds)\n",
    "\n",
    "    write_path = os.path.join(args.output_dir, \"output.csv\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)    \n",
    "    with open(write_path, 'w', encoding='utf8') as w:\n",
    "        print(\"writing prediction : {}\".format(write_path))\n",
    "        w.write(\"id,prediction\\n\")\n",
    "        for id, p in enumerate(total_preds):\n",
    "            w.write('{},{}\\n'.format(id,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPEE00qUnz1E"
   },
   "source": [
    "## 6.실행부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qZmwQenqnz1E"
   },
   "outputs": [],
   "source": [
    "data_dir = '/opt/ml/input/data/train_dataset'\n",
    "file_name = 'train_data.csv'\n",
    "test_file_name = 'test_data.csv'\n",
    "\n",
    "config = {}\n",
    "\n",
    "# 설정\n",
    "config['seed'] = 42\n",
    "config['device'] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config['data_dir'] = data_dir\n",
    "config['asset_dir'] = 'asset'\n",
    "config['model_dir'] = 'models'\n",
    "config['model_name'] = 'model.pt'\n",
    "config['output_dir'] = 'output'\n",
    "\n",
    "# 데이터\n",
    "config['max_seq_len'] = 30\n",
    "config['num_workers'] = 1\n",
    "\n",
    "# 모델\n",
    "config['hidden_dim'] = 64\n",
    "config['n_layers'] = 2\n",
    "config['dropout'] = 0.2\n",
    "\n",
    "# 훈련\n",
    "config['n_epochs'] = 100\n",
    "config['batch_size'] = 64\n",
    "config['lr'] = 5e-5\n",
    "config['clip_grad'] = 10\n",
    "config['log_steps'] = 50\n",
    "config['patience'] = 30\n",
    "\n",
    "\n",
    "### 중요 ###\n",
    "config['model'] = 'lstm'\n",
    "config['optimizer'] = 'adam'\n",
    "config['scheduler'] = 'cosine_warmup'\n",
    "\n",
    "args = easydict.EasyDict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setSeeds(seed = 42):\n",
    "    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)    \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "setSeeds(42)\n",
    "\n",
    "preprocess = Preprocess(args)\n",
    "preprocess.load_train_data(file_name)\n",
    "\n",
    "train_data = preprocess.get_train_data()\n",
    "train_data, valid_data = preprocess.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2y20u2i4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3232<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/opt/ml/code/wandb/run-20210524_172457-2y20u2i4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/opt/ml/code/wandb/run-20210524_172457-2y20u2i4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>49</td></tr><tr><td>train_loss</td><td>0.63742</td></tr><tr><td>train_auc</td><td>0.74035</td></tr><tr><td>train_acc</td><td>0.68082</td></tr><tr><td>valid_auc</td><td>0.72291</td></tr><tr><td>valid_acc</td><td>0.67164</td></tr><tr><td>_runtime</td><td>86</td></tr><tr><td>_timestamp</td><td>1621877183</td></tr><tr><td>_step</td><td>49</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███████████████████▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▃▃▂▁</td></tr><tr><td>train_auc</td><td>▁▁▁▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▄▅▆▆▇▇██████</td></tr><tr><td>valid_auc</td><td>▁▁▁▁▂▂▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇████████████████</td></tr><tr><td>valid_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▄▅▆▇▇▇▇████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">young-hill-35</strong>: <a href=\"https://wandb.ai/team-ikyo/P4-DKT/runs/2y20u2i4\" target=\"_blank\">https://wandb.ai/team-ikyo/P4-DKT/runs/2y20u2i4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2y20u2i4). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">super-shadow-36</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/team-ikyo/P4-DKT\" target=\"_blank\">https://wandb.ai/team-ikyo/P4-DKT</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/team-ikyo/P4-DKT/runs/gmpr78jx\" target=\"_blank\">https://wandb.ai/team-ikyo/P4-DKT/runs/gmpr78jx</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210524_173510-gmpr78jx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(gmpr78jx)</h1><iframe src=\"https://wandb.ai/team-ikyo/P4-DKT/runs/gmpr78jx\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff858ca58d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='P4-DKT', config=config, entity=\"team-ikyo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "v9qV6aXonz1E",
    "outputId": "0d36ac2e-7ca2-4fc0-cf4c-ea296bc40ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training: Epoch 1\n",
      "Training steps: 0 Loss: 0.692417562007904\n",
      "Training steps: 50 Loss: 0.6840407848358154\n",
      "TRAIN AUC : 0.5706521217682164 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5666883771558069 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 2\n",
      "Training steps: 0 Loss: 0.6862214207649231\n",
      "Training steps: 50 Loss: 0.6924540996551514\n",
      "TRAIN AUC : 0.5707625153638439 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.566964962839375 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 3\n",
      "Training steps: 0 Loss: 0.7004860639572144\n",
      "Training steps: 50 Loss: 0.692416787147522\n",
      "TRAIN AUC : 0.5710979420582505 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5674422961965007 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 4\n",
      "Training steps: 0 Loss: 0.6815810799598694\n",
      "Training steps: 50 Loss: 0.6838415265083313\n",
      "TRAIN AUC : 0.571644671579053 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5681025329895343 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 5\n",
      "Training steps: 0 Loss: 0.6949415802955627\n",
      "Training steps: 50 Loss: 0.6862828135490417\n",
      "TRAIN AUC : 0.5723872642625272 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.568981361693775 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 6\n",
      "Training steps: 0 Loss: 0.6926754713058472\n",
      "Training steps: 50 Loss: 0.7078460454940796\n",
      "TRAIN AUC : 0.5733982865281766 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5700743212498104 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 7\n",
      "Training steps: 0 Loss: 0.6875584125518799\n",
      "Training steps: 50 Loss: 0.7003850340843201\n",
      "TRAIN AUC : 0.5746439916824325 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5711940471623201 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 8\n",
      "Training steps: 0 Loss: 0.6917406916618347\n",
      "Training steps: 50 Loss: 0.6930617094039917\n",
      "TRAIN AUC : 0.5760360427921309 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5725591313425112 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 9\n",
      "Training steps: 0 Loss: 0.7017080783843994\n",
      "Training steps: 50 Loss: 0.7089024782180786\n",
      "TRAIN AUC : 0.5776695592141432 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.574240950740982 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 10\n",
      "Training steps: 0 Loss: 0.6867542266845703\n",
      "Training steps: 50 Loss: 0.6907169818878174\n",
      "TRAIN AUC : 0.5795056109392223 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5763064212489182 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 11\n",
      "Training steps: 0 Loss: 0.6878455877304077\n",
      "Training steps: 50 Loss: 0.6867403388023376\n",
      "TRAIN AUC : 0.5815521383658551 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5782157546774208 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 12\n",
      "Training steps: 0 Loss: 0.7022501826286316\n",
      "Training steps: 50 Loss: 0.6819038391113281\n",
      "TRAIN AUC : 0.5837682815268284 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5805176613342136 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 13\n",
      "Training steps: 0 Loss: 0.6965742707252502\n",
      "Training steps: 50 Loss: 0.698516845703125\n",
      "TRAIN AUC : 0.5862256694334912 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5830916926151622 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 14\n",
      "Training steps: 0 Loss: 0.6859776973724365\n",
      "Training steps: 50 Loss: 0.6959233283996582\n",
      "TRAIN AUC : 0.5888533899160249 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5857237176684719 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 15\n",
      "Training steps: 0 Loss: 0.6952184438705444\n",
      "Training steps: 50 Loss: 0.6844170093536377\n",
      "TRAIN AUC : 0.5916244235629119 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.588440502850617 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 16\n",
      "Training steps: 0 Loss: 0.696398138999939\n",
      "Training steps: 50 Loss: 0.6886947751045227\n",
      "TRAIN AUC : 0.5946340000143369 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5916167771522381 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 17\n",
      "Training steps: 0 Loss: 0.6868777275085449\n",
      "Training steps: 50 Loss: 0.6872795224189758\n",
      "TRAIN AUC : 0.5977235870088464 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5945566153050027 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 18\n",
      "Training steps: 0 Loss: 0.682781994342804\n",
      "Training steps: 50 Loss: 0.6983034610748291\n",
      "TRAIN AUC : 0.6009908404194957 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.5975856746460149 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 19\n",
      "Training steps: 0 Loss: 0.6877943277359009\n",
      "Training steps: 50 Loss: 0.6941701173782349\n",
      "TRAIN AUC : 0.6044240702151793 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6010965284035652 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 20\n",
      "Training steps: 0 Loss: 0.6820396184921265\n",
      "Training steps: 50 Loss: 0.6988538503646851\n",
      "TRAIN AUC : 0.6079523642260786 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6043218743587228 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 21\n",
      "Training steps: 0 Loss: 0.6819553971290588\n",
      "Training steps: 50 Loss: 0.686287522315979\n",
      "TRAIN AUC : 0.6115711456947325 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6076185972644783 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 22\n",
      "Training steps: 0 Loss: 0.6871113181114197\n",
      "Training steps: 50 Loss: 0.6858992576599121\n",
      "TRAIN AUC : 0.6151929048128189 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.611397114586772 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 23\n",
      "Training steps: 0 Loss: 0.6912109851837158\n",
      "Training steps: 50 Loss: 0.6891967058181763\n",
      "TRAIN AUC : 0.6189247266765958 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6152782362755507 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 24\n",
      "Training steps: 0 Loss: 0.691832423210144\n",
      "Training steps: 50 Loss: 0.6918518543243408\n",
      "TRAIN AUC : 0.6227046320645422 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6191325916078549 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 25\n",
      "Training steps: 0 Loss: 0.7003605365753174\n",
      "Training steps: 50 Loss: 0.6976995468139648\n",
      "TRAIN AUC : 0.6264152243906985 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6231252397819435 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 26\n",
      "Training steps: 0 Loss: 0.681491494178772\n",
      "Training steps: 50 Loss: 0.7046642899513245\n",
      "TRAIN AUC : 0.6302583221166015 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6273186356295893 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 27\n",
      "Training steps: 0 Loss: 0.689978837966919\n",
      "Training steps: 50 Loss: 0.686962366104126\n",
      "TRAIN AUC : 0.6341556792321634 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6315744863090086 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 28\n",
      "Training steps: 0 Loss: 0.6937505006790161\n",
      "Training steps: 50 Loss: 0.6912407875061035\n",
      "TRAIN AUC : 0.6380334059181334 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6353886921066015 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 29\n",
      "Training steps: 0 Loss: 0.6831233501434326\n",
      "Training steps: 50 Loss: 0.6911698579788208\n",
      "TRAIN AUC : 0.641875676519194 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6393055022706793 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 30\n",
      "Training steps: 0 Loss: 0.6827176809310913\n",
      "Training steps: 50 Loss: 0.6942216157913208\n",
      "TRAIN AUC : 0.6456827116019698 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6427940507311676 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 31\n",
      "Training steps: 0 Loss: 0.6906517744064331\n",
      "Training steps: 50 Loss: 0.6942943334579468\n",
      "TRAIN AUC : 0.6495278495691507 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6470855898858863 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 32\n",
      "Training steps: 0 Loss: 0.6913177967071533\n",
      "Training steps: 50 Loss: 0.687474250793457\n",
      "TRAIN AUC : 0.6533110083148103 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6517518580312454 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 33\n",
      "Training steps: 0 Loss: 0.6927112936973572\n",
      "Training steps: 50 Loss: 0.672516942024231\n",
      "TRAIN AUC : 0.6570383085627823 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.655543758531776 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 34\n",
      "Training steps: 0 Loss: 0.6943120956420898\n",
      "Training steps: 50 Loss: 0.6772613525390625\n",
      "TRAIN AUC : 0.6607302630091574 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6591126060616875 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 35\n",
      "Training steps: 0 Loss: 0.6956484317779541\n",
      "Training steps: 50 Loss: 0.6883639097213745\n",
      "TRAIN AUC : 0.6642208401272449 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6629045065622184 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 36\n",
      "Training steps: 0 Loss: 0.6808600425720215\n",
      "Training steps: 50 Loss: 0.6896501183509827\n",
      "TRAIN AUC : 0.6676444201331009 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6659558712002926 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 37\n",
      "Training steps: 0 Loss: 0.6888445615768433\n",
      "Training steps: 50 Loss: 0.6821715235710144\n",
      "TRAIN AUC : 0.6709846259548468 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6697031611066996 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 38\n",
      "Training steps: 0 Loss: 0.6828429102897644\n",
      "Training steps: 50 Loss: 0.679345965385437\n",
      "TRAIN AUC : 0.6742262936370389 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6728526690518464 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 39\n",
      "Training steps: 0 Loss: 0.683903694152832\n",
      "Training steps: 50 Loss: 0.6989715099334717\n",
      "TRAIN AUC : 0.6773904321506735 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6759575664028693 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 40\n",
      "Training steps: 0 Loss: 0.6891867518424988\n",
      "Training steps: 50 Loss: 0.6754833459854126\n",
      "TRAIN AUC : 0.6803322946483368 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6789286319715206 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 41\n",
      "Training steps: 0 Loss: 0.6858559250831604\n",
      "Training steps: 50 Loss: 0.6812664270401001\n",
      "TRAIN AUC : 0.6832431675352396 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.681475896895995 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 42\n",
      "Training steps: 0 Loss: 0.6873148679733276\n",
      "Training steps: 50 Loss: 0.6852411031723022\n",
      "TRAIN AUC : 0.6861574592048242 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6849287568811842 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 43\n",
      "Training steps: 0 Loss: 0.685234546661377\n",
      "Training steps: 50 Loss: 0.6838175058364868\n",
      "TRAIN AUC : 0.68879505004381 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6873109626074 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 44\n",
      "Training steps: 0 Loss: 0.6828192472457886\n",
      "Training steps: 50 Loss: 0.6900765895843506\n",
      "TRAIN AUC : 0.6914436140723713 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6894478100659345 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 45\n",
      "Training steps: 0 Loss: 0.6888051629066467\n",
      "Training steps: 50 Loss: 0.6832900047302246\n",
      "TRAIN AUC : 0.6938668692938725 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6917318724850777 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 46\n",
      "Training steps: 0 Loss: 0.6885271668434143\n",
      "Training steps: 50 Loss: 0.6838119626045227\n",
      "TRAIN AUC : 0.6960900154451779 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6937661155771273 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 47\n",
      "Training steps: 0 Loss: 0.6950991153717041\n",
      "Training steps: 50 Loss: 0.6807537078857422\n",
      "TRAIN AUC : 0.6983338397175424 ACC : 0.5214001327140013\n",
      "VALID AUC : 0.6957289817185784 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 48\n",
      "Training steps: 0 Loss: 0.6780133247375488\n",
      "Training steps: 50 Loss: 0.6875277757644653\n",
      "TRAIN AUC : 0.7004263552854104 ACC : 0.5215660252156602\n",
      "VALID AUC : 0.6976650815035557 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 49\n",
      "Training steps: 0 Loss: 0.6855498552322388\n",
      "Training steps: 50 Loss: 0.6872391700744629\n",
      "TRAIN AUC : 0.7024724967204499 ACC : 0.5215660252156602\n",
      "VALID AUC : 0.6993781283179129 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 50\n",
      "Training steps: 0 Loss: 0.6818110942840576\n",
      "Training steps: 50 Loss: 0.6915051937103271\n",
      "TRAIN AUC : 0.7043800120319094 ACC : 0.5215660252156602\n",
      "VALID AUC : 0.7007164461416296 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 51\n",
      "Training steps: 0 Loss: 0.6899938583374023\n",
      "Training steps: 50 Loss: 0.6827948689460754\n",
      "TRAIN AUC : 0.7062040980176023 ACC : 0.5217319177173192\n",
      "VALID AUC : 0.7026079353324828 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 52\n",
      "Training steps: 0 Loss: 0.6951197385787964\n",
      "Training steps: 50 Loss: 0.6869640350341797\n",
      "TRAIN AUC : 0.7079948784429761 ACC : 0.5217319177173192\n",
      "VALID AUC : 0.7040488575226845 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 53\n",
      "Training steps: 0 Loss: 0.6864886283874512\n",
      "Training steps: 50 Loss: 0.6770392656326294\n",
      "TRAIN AUC : 0.7096173665337568 ACC : 0.5217319177173192\n",
      "VALID AUC : 0.7050704401281216 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 54\n",
      "Training steps: 0 Loss: 0.6844491958618164\n",
      "Training steps: 50 Loss: 0.6901024580001831\n",
      "TRAIN AUC : 0.7112048396728776 ACC : 0.5217319177173192\n",
      "VALID AUC : 0.7060072626047235 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 55\n",
      "Training steps: 0 Loss: 0.6776145100593567\n",
      "Training steps: 50 Loss: 0.6930866241455078\n",
      "TRAIN AUC : 0.7128946557258269 ACC : 0.5218978102189781\n",
      "VALID AUC : 0.7073232751313782 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 56\n",
      "Training steps: 0 Loss: 0.6839854717254639\n",
      "Training steps: 50 Loss: 0.6891499161720276\n",
      "TRAIN AUC : 0.7143063924067733 ACC : 0.5220637027206371\n",
      "VALID AUC : 0.7081842595979693 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 57\n",
      "Training steps: 0 Loss: 0.6766519546508789\n",
      "Training steps: 50 Loss: 0.6914658546447754\n",
      "TRAIN AUC : 0.7156377325530421 ACC : 0.5220637027206371\n",
      "VALID AUC : 0.7092638359757675 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 58\n",
      "Training steps: 0 Loss: 0.6948622465133667\n",
      "Training steps: 50 Loss: 0.6832144260406494\n",
      "TRAIN AUC : 0.716888951872914 ACC : 0.5223954877239548\n",
      "VALID AUC : 0.7100043718382242 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 59\n",
      "Training steps: 0 Loss: 0.6926015019416809\n",
      "Training steps: 50 Loss: 0.6960905194282532\n",
      "TRAIN AUC : 0.7181502621158626 ACC : 0.5230590577305906\n",
      "VALID AUC : 0.7110259544436612 ACC : 0.5194029850746269\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 60\n",
      "Training steps: 0 Loss: 0.6834248304367065\n",
      "Training steps: 50 Loss: 0.676106870174408\n",
      "TRAIN AUC : 0.7193734143327501 ACC : 0.5232249502322495\n",
      "VALID AUC : 0.7115880479296223 ACC : 0.5194029850746269\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 61\n",
      "Training steps: 0 Loss: 0.6779543161392212\n",
      "Training steps: 50 Loss: 0.6940694451332092\n",
      "TRAIN AUC : 0.720566238638751 ACC : 0.5247179827471798\n",
      "VALID AUC : 0.7120832255243975 ACC : 0.517910447761194\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 62\n",
      "Training steps: 0 Loss: 0.6803902387619019\n",
      "Training steps: 50 Loss: 0.6782917976379395\n",
      "TRAIN AUC : 0.7215273577055777 ACC : 0.5268745852687459\n",
      "VALID AUC : 0.7125560978221107 ACC : 0.5223880597014925\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 63\n",
      "Training steps: 0 Loss: 0.6868630647659302\n",
      "Training steps: 50 Loss: 0.6726241111755371\n",
      "TRAIN AUC : 0.7226479464420122 ACC : 0.5296947577969475\n",
      "VALID AUC : 0.7134839981798878 ACC : 0.5238805970149254\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 64\n",
      "Training steps: 0 Loss: 0.6790873408317566\n",
      "Training steps: 50 Loss: 0.6886022686958313\n",
      "TRAIN AUC : 0.723664703439902 ACC : 0.533012607830126\n",
      "VALID AUC : 0.7139033377646524 ACC : 0.5238805970149254\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 65\n",
      "Training steps: 0 Loss: 0.6879961490631104\n",
      "Training steps: 50 Loss: 0.6784022450447083\n",
      "TRAIN AUC : 0.7246031041443917 ACC : 0.5351692103516921\n",
      "VALID AUC : 0.7144163595970772 ACC : 0.5328358208955224\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 66\n",
      "Training steps: 0 Loss: 0.6939610242843628\n",
      "Training steps: 50 Loss: 0.674263596534729\n",
      "TRAIN AUC : 0.7254999280401387 ACC : 0.5403118779031187\n",
      "VALID AUC : 0.7148936929542028 ACC : 0.5388059701492537\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 67\n",
      "Training steps: 0 Loss: 0.6743142604827881\n",
      "Training steps: 50 Loss: 0.6903876066207886\n",
      "TRAIN AUC : 0.7262595043537095 ACC : 0.5497677504976775\n",
      "VALID AUC : 0.7155985403413603 ACC : 0.5447761194029851\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 68\n",
      "Training steps: 0 Loss: 0.6766376495361328\n",
      "Training steps: 50 Loss: 0.687745213508606\n",
      "TRAIN AUC : 0.7271661986059086 ACC : 0.5547445255474452\n",
      "VALID AUC : 0.7163123098473425 ACC : 0.5462686567164179\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 69\n",
      "Training steps: 0 Loss: 0.6879795789718628\n",
      "Training steps: 50 Loss: 0.6815367937088013\n",
      "TRAIN AUC : 0.727793048953108 ACC : 0.5613802256138023\n",
      "VALID AUC : 0.716731649432107 ACC : 0.5582089552238806\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 70\n",
      "Training steps: 0 Loss: 0.6805175542831421\n",
      "Training steps: 50 Loss: 0.6751073598861694\n",
      "TRAIN AUC : 0.7285517430001802 ACC : 0.5668546781685467\n",
      "VALID AUC : 0.7171955996109955 ACC : 0.564179104477612\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 71\n",
      "Training steps: 0 Loss: 0.6840928792953491\n",
      "Training steps: 50 Loss: 0.6745134592056274\n",
      "TRAIN AUC : 0.7293068528396023 ACC : 0.579628400796284\n",
      "VALID AUC : 0.7177220046216576 ACC : 0.5746268656716418\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 72\n",
      "Training steps: 0 Loss: 0.6894361972808838\n",
      "Training steps: 50 Loss: 0.680274486541748\n",
      "TRAIN AUC : 0.7300070967311475 ACC : 0.5925680159256802\n",
      "VALID AUC : 0.7180030513646382 ACC : 0.5835820895522388\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 73\n",
      "Training steps: 0 Loss: 0.6819484233856201\n",
      "Training steps: 50 Loss: 0.6783482432365417\n",
      "TRAIN AUC : 0.730717266120801 ACC : 0.5990378234903783\n",
      "VALID AUC : 0.7185249953158876 ACC : 0.582089552238806\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 74\n",
      "Training steps: 0 Loss: 0.6840368509292603\n",
      "Training steps: 50 Loss: 0.6785203814506531\n",
      "TRAIN AUC : 0.7314435368740526 ACC : 0.6068347710683477\n",
      "VALID AUC : 0.7190781666830239 ACC : 0.5850746268656717\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 75\n",
      "Training steps: 0 Loss: 0.6774231791496277\n",
      "Training steps: 50 Loss: 0.6709319353103638\n",
      "TRAIN AUC : 0.7320755153952747 ACC : 0.6157929661579297\n",
      "VALID AUC : 0.7193012196536434 ACC : 0.5880597014925373\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 76\n",
      "Training steps: 0 Loss: 0.6719912886619568\n",
      "Training steps: 50 Loss: 0.6652551889419556\n",
      "TRAIN AUC : 0.732552931854287 ACC : 0.6189449236894492\n",
      "VALID AUC : 0.7199793006843265 ACC : 0.6\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 77\n",
      "Training steps: 0 Loss: 0.6745172739028931\n",
      "Training steps: 50 Loss: 0.6809003353118896\n",
      "TRAIN AUC : 0.7329577818937961 ACC : 0.6279031187790312\n",
      "VALID AUC : 0.7203495686155548 ACC : 0.6059701492537314\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 78\n",
      "Training steps: 0 Loss: 0.6692463159561157\n",
      "Training steps: 50 Loss: 0.6750683784484863\n",
      "TRAIN AUC : 0.7332547748538608 ACC : 0.635036496350365\n",
      "VALID AUC : 0.7207733692597318 ACC : 0.6238805970149254\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 79\n",
      "Training steps: 0 Loss: 0.6872453689575195\n",
      "Training steps: 50 Loss: 0.6738052368164062\n",
      "TRAIN AUC : 0.7340794183216975 ACC : 0.6420039814200398\n",
      "VALID AUC : 0.7209428895174026 ACC : 0.6358208955223881\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 80\n",
      "Training steps: 0 Loss: 0.6732134222984314\n",
      "Training steps: 50 Loss: 0.676743745803833\n",
      "TRAIN AUC : 0.7344128047748262 ACC : 0.6473125414731254\n",
      "VALID AUC : 0.721290852151569 ACC : 0.6432835820895523\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 81\n",
      "Training steps: 0 Loss: 0.6713000535964966\n",
      "Training steps: 50 Loss: 0.6630432605743408\n",
      "TRAIN AUC : 0.7350863601047912 ACC : 0.6531187790311878\n",
      "VALID AUC : 0.7215674378351371 ACC : 0.6477611940298508\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 82\n",
      "Training steps: 0 Loss: 0.6677560210227966\n",
      "Training steps: 50 Loss: 0.668387234210968\n",
      "TRAIN AUC : 0.7352542664477911 ACC : 0.6566025215660252\n",
      "VALID AUC : 0.721817257162231 ACC : 0.6388059701492538\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 83\n",
      "Training steps: 0 Loss: 0.6648356914520264\n",
      "Training steps: 50 Loss: 0.6758044958114624\n",
      "TRAIN AUC : 0.7357784430312251 ACC : 0.6595885865958858\n",
      "VALID AUC : 0.7219600110634274 ACC : 0.6477611940298508\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 84\n",
      "Training steps: 0 Loss: 0.6734097599983215\n",
      "Training steps: 50 Loss: 0.661230742931366\n",
      "TRAIN AUC : 0.736174525547405 ACC : 0.6624087591240876\n",
      "VALID AUC : 0.7219867774199018 ACC : 0.655223880597015\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 85\n",
      "Training steps: 0 Loss: 0.6649569272994995\n",
      "Training steps: 50 Loss: 0.671761691570282\n",
      "TRAIN AUC : 0.7365350968370193 ACC : 0.6662242866622429\n",
      "VALID AUC : 0.722080459667562 ACC : 0.6447761194029851\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 86\n",
      "Training steps: 0 Loss: 0.6692578792572021\n",
      "Training steps: 50 Loss: 0.6655640602111816\n",
      "TRAIN AUC : 0.7370357279332742 ACC : 0.6713669542136695\n",
      "VALID AUC : 0.7222633631034698 ACC : 0.6522388059701493\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 87\n",
      "Training steps: 0 Loss: 0.6766065359115601\n",
      "Training steps: 50 Loss: 0.6757673025131226\n",
      "TRAIN AUC : 0.7372363884200317 ACC : 0.6746848042468481\n",
      "VALID AUC : 0.7224730328958522 ACC : 0.6567164179104478\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 88\n",
      "Training steps: 0 Loss: 0.6472039818763733\n",
      "Training steps: 50 Loss: 0.6517682671546936\n",
      "TRAIN AUC : 0.7374914185797604 ACC : 0.6743530192435302\n",
      "VALID AUC : 0.7224507275987903 ACC : 0.6597014925373135\n",
      "\n",
      "Start Training: Epoch 89\n",
      "Training steps: 0 Loss: 0.6545861959457397\n",
      "Training steps: 50 Loss: 0.6705600023269653\n",
      "TRAIN AUC : 0.7378045680450795 ACC : 0.6756801592568016\n",
      "VALID AUC : 0.7225131824305637 ACC : 0.6597014925373135\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 90\n",
      "Training steps: 0 Loss: 0.654556155204773\n",
      "Training steps: 50 Loss: 0.6534612774848938\n",
      "TRAIN AUC : 0.7380298217104831 ACC : 0.6775049767750497\n",
      "VALID AUC : 0.7225845593811618 ACC : 0.6656716417910448\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 91\n",
      "Training steps: 0 Loss: 0.6601536273956299\n",
      "Training steps: 50 Loss: 0.6620394587516785\n",
      "TRAIN AUC : 0.7385535020190117 ACC : 0.6803251493032515\n",
      "VALID AUC : 0.7225221045493884 ACC : 0.6626865671641791\n",
      "\n",
      "Start Training: Epoch 92\n",
      "Training steps: 0 Loss: 0.6564822196960449\n",
      "Training steps: 50 Loss: 0.6467278003692627\n",
      "TRAIN AUC : 0.7387894531657101 ACC : 0.679163901791639\n",
      "VALID AUC : 0.7226024036188115 ACC : 0.6671641791044776\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 93\n",
      "Training steps: 0 Loss: 0.6503366231918335\n",
      "Training steps: 50 Loss: 0.6717678904533386\n",
      "TRAIN AUC : 0.7389809601375454 ACC : 0.681320504313205\n",
      "VALID AUC : 0.7227719238764821 ACC : 0.6716417910447762\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 94\n",
      "Training steps: 0 Loss: 0.6541410684585571\n",
      "Training steps: 50 Loss: 0.6537812948226929\n",
      "TRAIN AUC : 0.7395030413380455 ACC : 0.681320504313205\n",
      "VALID AUC : 0.7229057556588538 ACC : 0.673134328358209\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 95\n",
      "Training steps: 0 Loss: 0.6492453813552856\n",
      "Training steps: 50 Loss: 0.6344279050827026\n",
      "TRAIN AUC : 0.7396138760669221 ACC : 0.6823158593231586\n",
      "VALID AUC : 0.7229503662529777 ACC : 0.6761194029850747\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 96\n",
      "Training steps: 0 Loss: 0.6675565242767334\n",
      "Training steps: 50 Loss: 0.6522634029388428\n",
      "TRAIN AUC : 0.7398092980963447 ACC : 0.6838088918380889\n",
      "VALID AUC : 0.722905755658854 ACC : 0.6761194029850747\n",
      "\n",
      "Start Training: Epoch 97\n",
      "Training steps: 0 Loss: 0.6629478931427002\n",
      "Training steps: 50 Loss: 0.6766805648803711\n",
      "TRAIN AUC : 0.7398948779467012 ACC : 0.6821499668214996\n",
      "VALID AUC : 0.7230128210847512 ACC : 0.6761194029850747\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 98\n",
      "Training steps: 0 Loss: 0.6685456037521362\n",
      "Training steps: 50 Loss: 0.6543511152267456\n",
      "TRAIN AUC : 0.739927356382178 ACC : 0.6821499668214996\n",
      "VALID AUC : 0.7230306653224008 ACC : 0.6746268656716418\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 99\n",
      "Training steps: 0 Loss: 0.6607905626296997\n",
      "Training steps: 50 Loss: 0.6353657841682434\n",
      "TRAIN AUC : 0.7402753002325324 ACC : 0.681486396814864\n",
      "VALID AUC : 0.7231377307482981 ACC : 0.673134328358209\n",
      "\n",
      "saving model ...\n",
      "Start Training: Epoch 100\n",
      "Training steps: 0 Loss: 0.648962676525116\n",
      "Training steps: 50 Loss: 0.6430833339691162\n",
      "TRAIN AUC : 0.7404201022216021 ACC : 0.681320504313205\n",
      "VALID AUC : 0.7230128210847511 ACC : 0.6761194029850747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(args, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFY0zXGFnz1F"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcTCBhrZnz1G"
   },
   "outputs": [],
   "source": [
    "preprocess = Preprocess(args)\n",
    "preprocess.load_test_data(test_file_name)\n",
    "test_data = preprocess.get_test_data()\n",
    "inference(args, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3강_lstm_baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
